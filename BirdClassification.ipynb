{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRzPDiVzsyGz",
        "outputId": "153566fa-0e6a-4c9d-ca12-ff55123e6797"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS5Ch2HbC6Gb"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "checkpoints = '/content/drive/MyDrive/colab_files/DenseNet/'\n",
        "if not os.path.exists(checkpoints):\n",
        "    os.makedirs(checkpoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYJ6ZE1CuGth"
      },
      "source": [
        "os.chdir('drive/MyDrive') # Need to change directory to the google drive before downloading datasaet\n",
        "if not os.path.exists('birds21sp'):\n",
        "    !mkdir birds21sp\n",
        "    os.chdir('birds21sp')\n",
        "    !wget https://pjreddie.com/media/files/birds/train.tar\n",
        "    !wget https://pjreddie.com/media/files/birds/test.tar\n",
        "    !wget https://pjreddie.com/media/files/birds/names.txt\n",
        "    !tar xf train.tar\n",
        "    !tar xf test.tar\n",
        "    !mkdir testing\n",
        "    !mv test testing\n",
        "    os.chdir('..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sNQBsQiszKj"
      },
      "source": [
        "def get_bird_data(augmentation=0, validation = False):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(256, padding=8, padding_mode='edge'), # Take 256x256 crops from padded images\n",
        "        transforms.RandomHorizontalFlip(),    # 50% of time flip image along y-axis\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    \n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.ImageFolder(root='birds21sp/train', transform=transform_train)\n",
        "\n",
        "    if validation:      \n",
        "      # Make a validation set\n",
        "      validation_len = len(trainset) // 10\n",
        "      train_len = len(trainset) - validation_len\n",
        "      subsets = torch.utils.data.random_split(trainset, [train_len, validation_len])\n",
        "\n",
        "      trainloader = torch.utils.data.DataLoader(subsets[0], batch_size=16,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "      valloader = torch.utils.data.DataLoader(subsets[1], batch_size=16,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "    else:\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "      valloader = None\n",
        "      validation_len = 0\n",
        "\n",
        "    testset = torchvision.datasets.ImageFolder(root='birds21sp/testing', transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n",
        "    classes = open(\"birds21sp/names.txt\").read().strip().split(\"\\n\")\n",
        "    class_to_idx = trainset.class_to_idx\n",
        "    idx_to_class = {int(v): int(k) for k, v in class_to_idx.items()}\n",
        "    idx_to_name = {k: classes[v] for k,v in idx_to_class.items()}\n",
        "    if validation:\n",
        "      return {'train': trainloader, 'val' : valloader, 'val_len': validation_len, 'test': testloader, 'to_class': idx_to_class, 'to_name':idx_to_name}\n",
        "    return {'train': trainloader, 'test': testloader, 'to_class': idx_to_class, 'to_name':idx_to_name}\n",
        "\n",
        "data = get_bird_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llHSMv7js5yA"
      },
      "source": [
        "# Check Images\n",
        "dataiter = iter(data['train'])\n",
        "images, labels = dataiter.next()\n",
        "images = images[:8]\n",
        "print(images.size())\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(\"Labels:\" + ', '.join('%9s' % data['to_name'][labels[j].item()] for j in range(8)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5_LglWCs9Iu"
      },
      "source": [
        "def train(net, dataloader, epochs=1, start_epoch=0, lr=0.01, momentum=0.9, decay=0.0005, \n",
        "          verbose=1, print_every=10, state=None, schedule={}, checkpoint_path=None):\n",
        "    net.to(device)\n",
        "    net.train()\n",
        "    losses = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.Adam(net.parameters, lr=lr, weight_decay=decay)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "\n",
        "    # Load previous training state\n",
        "    if state:\n",
        "        net.load_state_dict(state['net'])\n",
        "        optimizer.load_state_dict(state['optimizer'])\n",
        "        start_epoch = state['epoch']\n",
        "        losses = state['losses']\n",
        "\n",
        "    # Fast forward lr schedule through already trained epochs\n",
        "    for epoch in range(start_epoch):\n",
        "        if epoch in schedule:\n",
        "            print (\"Learning rate: %f\"% schedule[epoch])\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = schedule[epoch]\n",
        "\n",
        "    for epoch in tqdm(range(start_epoch, epochs)):\n",
        "        sum_loss = 0.0\n",
        "\n",
        "        # Update learning rate when scheduled\n",
        "        if epoch in schedule:\n",
        "            print (\"Learning rate: %f\"% schedule[epoch])\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = schedule[epoch]\n",
        "\n",
        "        for i, batch in enumerate(dataloader, 0):\n",
        "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()  # autograd magic, computes all the partial derivatives\n",
        "            optimizer.step() # takes a step in gradient direction\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            sum_loss += loss.item()\n",
        "\n",
        "            if i % print_every == print_every-1:    # print every 10 mini-batches\n",
        "                if verbose:\n",
        "                  print('[%d, %5d] loss: %.3f' % (epoch, i + 1, sum_loss / print_every))\n",
        "                sum_loss = 0.0\n",
        "        if checkpoint_path:\n",
        "            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n",
        "            torch.save(state, checkpoint_path + 'checkpoint-%d.pkl'%(epoch+1))\n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTji55GvU2mc"
      },
      "source": [
        "def getResNet():\n",
        "  model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True)\n",
        "  model.fc = nn.Linear(2048, 555) # This will reinitialize the layer as well\n",
        "  return model\n",
        "\n",
        "def getResNext():\n",
        "  model = torch.hub.load('pytorch/vision:v0.9.0', 'resnext101_32x8d', pretrained=True)\n",
        "  model.fc = nn.Linear(2048, 555) # This will reinitialize the layer as well\n",
        "  return model\n",
        "\n",
        "def getDenseNet():\n",
        "  model = torch.hub.load('pytorch/vision:v0.9.0', 'densenet201', pretrained=True)\n",
        "  model.classifier = nn.Linear(1920, 555) # This will reinitialize the layer as well\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciUXCJ5LvN1p"
      },
      "source": [
        "# Training from checkpoints, \n",
        "model = getDenseNet()\n",
        "state = torch.load(checkpoints + 'checkpoint-13.pkl')\n",
        "# state = None # If we don't want to train from checkpoints\n",
        " losses = train(model, data['train'], epochs=15, schedule={0:.01, 7:.001, 13: .0001}, lr=.01, print_every=10, checkpoint_path=checkpoints, state=state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVyblLspSFnU"
      },
      "source": [
        "def smooth(x, size):\n",
        "  return np.convolve(x, np.ones(size)/size, mode='valid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1VsrjMp_Lvo"
      },
      "source": [
        "state = torch.load(checkpoints + 'checkpoint-13.pkl', map_location=torch.device('cpu'))\n",
        "plt.plot(smooth(state['losses'], 200), label = '13 epoch')\n",
        "state = torch.load(checkpoints + 'checkpoint-6.pkl', map_location=torch.device('cpu'))\n",
        "plt.plot(smooth(state['losses'], 200), label = '6 epoch')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Z5xM_YUWDy"
      },
      "source": [
        "def count_accurate(output, labels):\n",
        "  classification = torch.argmax(output, 1)\n",
        "  return torch.sum(classification == labels).item()\n",
        "  \n",
        "# Check accuracy with validation set\n",
        "def getValidationAcc(model, data):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  accurate = 0\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for i, in_out in tqdm(enumerate(data['val'], 0)):\n",
        "    input, labels = in_out\n",
        "    input = input.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model(input)\n",
        "      loss = loss_func(output, labels)\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "    accurate += count_accurate(output, labels)\n",
        "\n",
        "  accurate /= data['val_len']\n",
        "  print(\"Validation loss: \" + str(epoch_loss))\n",
        "  print(\"Validation acc: \" + str(accurate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH3jOjwkvAZN"
      },
      "source": [
        "def predict(net, dataloader, ofname):\n",
        "    out = open(ofname, 'w')\n",
        "    out.write(\"path,class\\n\")\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(dataloader, 0):\n",
        "            if i%100 == 0:\n",
        "                print(i)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            fname, _ = dataloader.dataset.samples[i]\n",
        "            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n",
        "    out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_doagLCS9u0"
      },
      "source": [
        "# Load model from checkpoint\n",
        "model = torch.hub.load('pytorch/vision:v0.9.0', 'resnext101_32x8d', pretrained=True)\n",
        "model.fc = nn.Linear(2048, 555) # This will reinitialize the layer as well\n",
        "state = torch.load(checkpoints + 'checkpoint-10.pkl')\n",
        "model.load_state_dict(state['net'])\n",
        "\n",
        "# getValidationAcc(model, data)\n",
        "# predict(model, data['test'], checkpoints + \"d.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}